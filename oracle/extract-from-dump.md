# Extract data from Oracle dump files

These instructions cover the specific case where of extracting data out of Oracle dump files generated by a version newer than version 11 (since as of this writing the only version Oracle available as a standalone EC2 instance through the AWS marketplace is version 11). That said, most of the things covered here should be generically useful for extracting data out of Oracle data dumps.

Despite there only being images for Oracle version 11 or older in the AWS
marketplace, you can get an RDS instance with version 12 on it (in fact, I
think that might be the only way to do it). Because you do not have
shell access to an RDS instance, you have to jump through some fun hoops to get
the dumps loaded into RDS and then dumped back out as CSV.

1. Launch an EC2 instance from an Oracle version 11 AMI. Since this will
function mainly as a way to connect to the RDS instance, it doesn't really
matter how big the hard drive is or how much power it has.

2. ssh into that instance and go through the setup. It'll take a while. Make
sure you remember what name you chose as the System Identifier (or SID) because
we'll need it later.

3. Launch an RDS instance with Oracle version 12 Enterprise Edition on it. Make
sure that it is in the same VPC as the version 11 instance and that the version
11 instance is able to connect directly to it. One easy way of doing this is
adding a rule to the security group that allows instances from within the
security group that that version 11 instance is in to attach to the version 12
RDS instance via the port that Oracle is listening on. That way nothing is open
to the public internet. It does matter how much space the RDS instance has
since this is where the dump files will be loaded.

4. When you're going through the setup, make note of the name of the Master
User that you are prompted to make and that user's password. Also, make note of
the DB Name that AWS chooses for it. This will be used as the SID for any
commands that we are using below. In the one that I launched, it chose "ORCL"
as the SID.

5. `rsync` the dump files to the version 11 instance. This is the command that
I used since I wanted to be able to use a PEM file to connect (I also just
zipped up all the files and sent them in one go):

```
rsync -Pav -e 'ssh -i /home/eric/.ssh/oracle.pem' <filename to send> ec2-user@<hostname or IP of version 11 instance>:/home/ec2-user
```

6. Unpack the dump files on the Version 11 instance and modify the permissions
on them so that the `oracle` user can access them.

```
sudo chmod -R oracle <name of directory where the files are extracted>
sudo mv <name of the directory> /home/oracle
```

7. The next part is kinda weird and is part of the genius of Oracle's total
lock-in model, I guess. In order for files to be loaded into a database using
the "Data Pump" tool (which, as far as I can tell is the standard way of doing
these things), they need to be in a very specific directory down in the
bowels of the Oracle installation itself. And, in order for the RDS instance to
be able to load them in that way, we need to use an Oracle SQL command to copy
files from the version 11 machine to the version 12 RDS instance.

8. As the oracle user, copy the dump files that we unpacked and moved to the
oracle user's home direcory to the Data Pump directory. This is where the SID
from step 2 comes in:

```
sudo su - oracle
cp <name of the directory with dump files>/* /u01/app/oracle/admin/<SID from step 2>/dpdump/
```

9. Login to the Oracle database using `sqlplus` and create a database link.
You'll need to know the name and password of the Master User that you chose in
step 4 above, the SID that AWS chose for the RDS instance (it's in the
"details" panel for that RDS instance in the AWS console under the name "DB
Name") and the host name and port number that Oracle is listening on (in my
case it was 1521) for the RDS instance.

```
sqlplus / AS SYSDBA

# and then inside Oracle shell. You might need to put this all on one line.

create database link to_rds connect to <master user name> identified by <master user password>
using '(
  DESCRIPTION=(
    ADDRESS=(
      PROTOCOL=TCP)
    (HOST=<hostname for RDS instance>)
    (PORT=<listener port>)
  )(CONNECT_DATA=(SID=<SID for RDS instance>)))';
```

10. Use the `DBMS_FILE_TRANSFER.PUT_FILE` command to copy files to the RDS
instance. As far as I know, you have to do this one at a time.

```
BEGIN
  DBMS_FILE_TRANSFER.PUT_FILE(
    source_directory_object       => 'DATA_PUMP_DIR',
    source_file_name              => '<name of dump file>',
    destination_directory_object  => 'DATA_PUMP_DIR',
    destination_file_name         => '<name of dump file>',
    destination_database          => 'to_rds'
  );
END;
/
```

The duplicated semicolons and the slash at the end are needed. I also had to
input that command as well as the one above manually because copy paste totally
borked the whole thing. I'm sure there's a way of sticking all of this into a
SQL script or something but I have yet to figure that out.

11. Create necessary tablespaces. There is really no way of knowing in
advance what tablespaces will be needed until you go on to the import step and try to import a dump file and it fails with something like:

```
ORA-00959: tablespace 'SOME_TABLESPACE' does not exist
```

To fix it, login to the version 12 instance from the version 11
instance using the `sqlplus` tool and then create the tablespaces:

```
sqlplus <rds master user>/<password>@<hostname>:<port>/<sid>

# and then in the SQL shell:

CREATE TABLESPACE <tablespace name>;
```

12. There will more than likely be users that the dump file will
require. You'll know because when you run the `impdp` command, it'll
not be able to create any tables because a "user" or "schema" doesn't
exist. The errors will look something like:

```
ORA-39083: Object type SOME_TABLE failed to create with error:
ORA-31625: Schema SOME_USER_NAME is needed to import this object, but is unaccessible
ORA-01435: user does not exist
```

We can remap those by adding a `remap_schema` argument to the
end of the `impdp` command. It'll take the form of
`remap_schema=<missing user name>:<master user name>`

12. Now, finally, use the Data Pump import tool (`impdp`) on the version 11 instance to
import the files that you just copied to the version 12 instance:

```
impdp <master user name>/<master user password>@<rds hostname>:<port number>/<SID> DUMPFILE=<name of dump file> full=y remap_schema=<missing user name>:<master user name>
```

13. Create directory object and export / dump procedure on version 11
instance. This is the real magic of the whole process. Included with
this file should be a SQL file called `dump2csv.sql`. In that file there is a directory object that is created in the first line. Change it if you need to and make sure that directory exists on the file system:

```
-- Change this line if you need to --
CREATE OR REPLACE DIRECTORY U01_EXPORTS as '/u01/exports';
```

Login to the version 11 SQL shell using `sqlplus`, create the
procedure, and then execute it. This command assumes the sql file with
the procedure in it is called `dump2csv.sql` and is in the directory
you are in when you connect to the database with `sqlplus`:

```
sqlplus / as sysdba

# Then in the SQL shell:

@dump2csv.sql

# It'll exit when it's done creating so you'll need to log back in

sqlplus / as sysdba

# Once again in the SQL shell:

exec dump_table_to_csv('NAME_OF_TABLE');

# If you're not sure what the tables are, you can query the remote database to find out:

select table_name from all_tables@to_rds where owner='<master user name>';

```

14. Repeat as necessary.

### dump_table_to_csv procedure

```
-- Ensure that the named DIRECTORY object is defined for later use when exporting data.
CREATE OR REPLACE DIRECTORY U01_EXPORTS as '/u01/exports';

-- Create the procedure used to dump a table to CSV.
create or replace procedure dump_table_to_csv( p_tname in varchar2 )
  is
      l_output        utl_file.file_type;
      l_theCursor     integer default dbms_sql.open_cursor;
      l_columnValue   varchar2(4000);
      l_status        integer;
      l_query         varchar2(1000)
                       default 'select * from ' || p_tname '@to_rds';
      l_colCnt        number := 0;
      l_separator     varchar2(1);
      l_descTbl       dbms_sql.desc_tab;
  begin
      -- Note that this takes the name of a directory object. Not the name of a directory!
      l_output := utl_file.fopen( 'U01_EXPORTS', p_tname || '.csv', 'w', 32767 );
      execute immediate 'alter session set nls_date_format=''yyyy-mm-dd"T"hh24:mi:ss'' ';

      dbms_sql.parse(  l_theCursor,  l_query, dbms_sql.native );
      dbms_sql.describe_columns( l_theCursor, l_colCnt, l_descTbl );

      for i in 1 .. l_colCnt loop
          utl_file.put( l_output, l_separator || '"' || l_descTbl(i).col_name || '"');
          dbms_sql.define_column( l_theCursor, i, l_columnValue, 4000 );
          l_separator := ',';
      end loop;
      utl_file.new_line( l_output );

      l_status := dbms_sql.execute(l_theCursor);

      while ( dbms_sql.fetch_rows(l_theCursor) > 0 ) loop
          l_separator := '';
          for i in 1 .. l_colCnt loop
              dbms_sql.column_value( l_theCursor, i, l_columnValue );
              utl_file.put( l_output, l_separator || '"' || replace(l_columnValue, '"', '""') || '"');
              l_separator := ',';
          end loop;
          utl_file.new_line( l_output );
      end loop;
      dbms_sql.close_cursor(l_theCursor);
      utl_file.fclose( l_output );

      execute immediate 'alter session set nls_date_format=''dd-MON-yy'' ';
  exception
      when others then
          execute immediate 'alter session set nls_date_format=''dd-MON-yy'' ';
          raise;
  end;
/
quit;
```